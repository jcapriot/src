%\def\SEPCLASSLIB{../../../../sepclasslib}
\def\CAKEDIR{.}

\title{Adjoint operators}
\author{Jon Claerbout}
\label{paper:conj}
\maketitle

A great many of the calculations
we do in science and engineering
are really matrix multiplication in disguise.
The first goal of this chapter is to unmask the disguise
by showing many examples.
Second, we see how the
\bx{adjoint} operator (matrix transpose)
back-projects information from data to the underlying model.

\par
Geophysical modeling calculations
generally use linear operators that predict data from models.
Our usual task is to find the inverse of these calculations;
i.e., to find models (or make maps) from the data.
Logically, the adjoint is the first step
and a part of all subsequent steps in this \bx{inversion} process.
Surprisingly, in practice the adjoint sometimes does a better job
than the inverse!
This is because the adjoint operator tolerates imperfections
in the data and does not demand that the data provide full information.

\par
Using the methods of this chapter,
you will find that
once you grasp the relationship between operators in general
and their adjoints,
you can obtain the adjoint just
as soon as you have learned how to code
the modeling operator.

\par
If you will permit me a poet's license with words,
I will offer you the following table
of \bx{operator}s and their \bx{adjoint}s:

\begin{tabular}{p{2em}lp{1em}l}
&\bx{matrix multiply}		&&conjugate-transpose matrix multiply \\
&convolve 			&&crosscorrelate	\\
&truncate			&&zero pad	\\
&replicate, scatter, spray	&&sum or stack	\\
&spray into neighborhood	&&sum in bins	\\
&derivative (slope) 		&&negative derivative	\\
&causal integration 		&&anticausal integration	\\
&add functions			&&do integrals	\\
&assignment statements		&&added terms	\\
&plane-wave superposition	&&slant stack / beam form	\\
&superpose on a curve		&&sum along a curve \\
&stretch			&&squeeze	\\
&upward continue		&&downward continue \\
&hyperbolic modeling	 	&&normal moveout and CDP stack	\\
&diffraction modeling	 	&&imaging by migration	\\
&ray tracing			&&\bx{tomography}
\end{tabular}
\par
The left column above is often called ``\bx{modeling},''
and the adjoint operators on the right are often
used in ``data \bx{processing}.''

\par
The adjoint operator is sometimes called
the ``\bx{back projection}'' operator
because information propagated in one direction (earth to data) is projected
backward (data to earth model).
For complex-valued operators,
the transpose goes together with a complex conjugate.
In \bx{Fourier analysis}, taking the complex conjugate
of $\exp(i\omega t)$ reverses the sense of time.
With more poetic license, I say that adjoint operators
{\it undo}
the time and phase shifts of modeling operators.
The inverse operator does this too,
but it also divides out the color.
For example, when linear interpolation is done,
then high frequencies are smoothed out,
so inverse interpolation must restore them.
You can imagine the possibilities for noise amplification.
That is why adjoints are safer than inverses.

\par
Later in this chapter we relate
adjoint operators to inverse operators.
Although inverse operators are more well known than adjoint operators,
the inverse is built upon the adjoint
so the adjoint is a logical place to start.
Also, computing the inverse is a complicated process
fraught with pitfalls whereas
the computation of the adjoint is easy.
It's a natural companion to the operator itself.

%Often, the adjoint is all you need for imaging
%and when it isn't
%The adjoint is often the best place to end too,
%because inverses of giant operators (and these are giants)
%are notoriously ill-behaved.

%\par
%When the adjoint operator is
%{\it not}
%an adequate approximation to the inverse,
%then you apply the techniques of fitting and optimization
%that are not within the scope of this book.
%The computed adjoint should be, and generally can be,
%exact (within machine precision).

\par
Much later in this chapter is a formal definition of adjoint operator.
Throughout the chapter we handle an adjoint operator as a matrix transpose,
but we hardly ever write down any matrices or their transposes.
Instead, we always prepare two subroutines,
one that performs
$\bold y =\bold A \bold x$
and another that performs
$\tilde{\bold x} =\bold A' \bold y$.
So we need a test that the two subroutines
really embody the essential aspects of matrix transposition.
Although the test is an elegant and useful test
and is itself a fundamental definition,
curiously,
that definition
does not help construct
adjoint operators,
so we postpone a formal definition of adjoint
until after we have seen many examples.

\section{FAMILIAR OPERATORS}

\par
\sx{matrix multiply}
The operation $ y_i = \sum_j b_{ij} x_j$ is the multiplication
of a matrix $\bold B$ by a vector $\bold x$.
The adjoint operation is
$\tilde x_j = \sum_i b_{ij} y_i$.
The operation adjoint to multiplication by a matrix
is multiplication by the transposed matrix
(unless the matrix has complex elements,
in which case we need the complex-conjugated transpose).
The following \bx{pseudocode} does matrix multiplication
$\bold y=\bold B\bold x$ and multiplication by the transpose
$\tilde \bold x = \bold B' \bold y$:

\par
\vbox{\begin{tabbing}
indent \= atechars \= atechars \=  atechars \= \kill
\>if operator itself	\\
\>	\>then erase y	\\
\>if adjoint	\\
\>	\>then erase x	\\
\>do iy = 1, ny \{	\\
\>do ix = 1, nx \{	\\
\>	\>if operator itself	\\
\>	\>	\>y(iy) = y(iy) + b(iy,ix) $\times$ x(ix)	\\
\>	\>if adjoint	\\
\>	\>	\>x(ix) = x(ix) + b(iy,ix) $\times$ y(iy)	\\
\>	\>\}\}
\end{tabbing} }
\par

\noindent
Notice that the ``bottom line'' in the program is that $x$ and $y$
are simply interchanged.
The above example is a prototype of many to follow,
so observe carefully the similarities and differences between the operation
and its adjoint.

\par
A formal subroutine
%\footnote{
%	The programming language used in this book is Ratfor,
%	a dialect of Fortran.
%	For more details, see Appendix A.}
for \bx{matrix multiply} and its adjoint is found below.
The first step is a subroutine, \texttt{adjnull()},
for optionally erasing the output.
With the option {\tt add=true}, results accumulate like {\tt y=y+B*x}.
\moddex{adjnull}{erase output}{26}{48}{api/c}
The subroutine \texttt{matmult()}
for matrix multiply and its adjoint
exhibits the style that we will use repeatedly.
\opdex{matmult}{matrix multiply}{33}{45}{user/pwd}

% NEW
\par
Sometimes a matrix operator reduces to a simple row or a column.

\par\noindent
A {\bf row} \quad\ \ is a summation operation.
\par\noindent
A {\bf column}       is an impulse response.
\vspace{.2in}
\par\noindent
If the inner loop of a matrix multiply ranges within a
\par\noindent
{\bf row,} \quad\ \ the operator is called {\it sum} or {\it pull}.
\par\noindent
{\bf column,}       the operator is called {\it spray} or {\it push}.

\par
A basic aspect of adjointness is that the
adjoint of a row matrix operator is a column matrix operator.
For example,
the row operator $[a,b]$
\begin{equation}
y \eq
\left[ \ a \ b \ \right]
\left[
\begin{array}{l}
	x_1 \\
	x_2
\end{array}
\right]
\eq
a x_1 + b x_2
\end{equation}
has an adjoint that is two assignments:
\begin{equation}
	\left[
	\begin{array}{l}
		\hat x_1 \\
		\hat x_2
	\end{array}
	\right]
	\eq
	\left[
	\begin{array}{l}
		a \\
		b
	\end{array}
	\right]
	\ y
\end{equation}
\par
\boxit{
The adjoint of a sum of $N$ terms
is a collection of $N$ assignments.
}

\subsection{Adjoint derivative}
Given a sam\-pled sig\-nal,
its time \bx{derivative} can be esti\-mated
by con\-vo\-lu\-tion with the fil\-ter $(1,-1)/\Delta t$,
expressed as the matrix-multiply below:
\begin{equation}
\left[ \begin{array}{c}
	y_1 \\
	y_2 \\
	y_3 \\
	y_4 \\
	y_5 \\
	y_6
	\end{array} \right]
\eq
\left[ \begin{array}{cccccc}
	-1& 1& .& .& .& . \\
	 .&-1& 1& .& .& . \\
	 .& .&-1& 1& .& . \\
	 .& .& .&-1& 1& . \\
	 .& .& .& .&-1& 1 \\
	 .& .& .& .& .& 0
	\end{array} \right] \
\left[ \begin{array}{c}
	x_1 \\
	x_2 \\
	x_3 \\
	x_4 \\
	x_5 \\
	x_6
	\end{array} \right]
 \label{eqn:ruff1}
\end{equation}
Technically the output should be {\tt n-1} points long,
but I appended a zero row,
a small loss of logical purity,
so that the size of the output vector will match that of the input.
This is a convenience for plotting
and for simplifying the assembly of other operators building on this one.
\par
The \bx{filter impulse response} is seen in any column
in the middle of the matrix, namely $(1,-1)$.
In the transposed matrix,
the filter-impulse response
is time-reversed to $(-1,1)$.
So, mathematically,
we can say that the adjoint of the time derivative operation
is the negative time derivative.
This corresponds also to the fact that
the complex conjugate of $-i\omega$ is $i\omega$.
We can also speak of the adjoint of the boundary conditions:
we might say that the adjoint of ``no boundary condition''
is a ``specified value'' boundary condition.
\par
A complicated way to think about the adjoint of equation
(\ref{eqn:ruff1}) is to note that it is the negative of the derivative
and that something must be done about the ends.
A simpler way to think about it
is to apply the idea that the adjoint of a sum of $N$ terms
is a collection of $N$ assignments.
This is done in subroutine \texttt{igrad1()},
which implements equation~(\ref{eqn:ruff1})
and its adjoint.
\moddex{igrad1}{first difference}{25}{40}{api/c}
\par
\noindent
Notice that the do loop in the code
covers all the outputs for the operator itself,
and that in the adjoint operation it gathers all the inputs.
This is natural because in switching from operator
to adjoint, the outputs switch to inputs.
\par
As you look at the code,
think about matrix elements being $+1$ or $-1$ and
think about the forward operator
``pulling'' a sum into {\tt yy(i)}, and
think about the adjoint operator
``pushing'' or ``spraying'' the impulse {\tt yy(i)} back into {\tt xx()}.

\par
You might notice that you can simplify the program
by merging the ``erase output'' activity with the calculation itself.
We will not do this optimization however because in many applications
we do not want to include the ``erase output'' activity.
This often happens when we build complicated operators from simpler ones.


\subsection{Zero padding is the transpose of truncation}
Surrounding a dataset by zeros
(\bx{zero pad}ding)
is adjoint to throwing away the extended data
(\bx{truncation}).
Let us see why this is so.
Set a signal in a vector $\bold x$, and
then to make a longer vector $\bold y$,
add some zeros at the end of $\bold x$.
This zero padding can be regarded as the matrix multiplication
\begin{equation}
\bold y\eq
 \left[
  \begin{array}{c}
   \bold I \\
   \bold 0
  \end{array}
 \right]
 \
 \bold x
\end{equation}
The matrix is simply an identity matrix $\bold I$
above a zero matrix $\bold 0$.
To find the transpose to zero-padding, we now transpose the matrix
and do another matrix multiply:
\begin{equation}
\tilde {\bold x} \eq
 \left[
  \begin{array}{cc}
   \bold I & \bold 0
  \end{array}
 \right]
\
\bold y
\end{equation}
So the transpose operation to zero padding data
is simply {\it truncating} the data back to its original length.
Subroutine \texttt{zpad1()} below
pads zeros on both ends of its input.
Subroutines for two- and three-dimensional padding are in the
library named {\tt zpad2()} and {\tt zpad3()}.
\opdex{zpad1}{zero pad 1-D}{21}{32}{user/gee}


\subsection{Adjoints of products are reverse-ordered products of adjoints}
Here we examine an example of the general idea that
adjoints of products are reverse-ordered products of adjoints.
For this example we use the Fourier transformation.
No details of \bx{Fourier transformation} are given here
and we merely use it as an example of a square matrix $\bold F$.
We denote the complex-conjugate transpose (or \bx{adjoint}) matrix
with a prime,
i.e.,~$\bold F'$.
The adjoint arises naturally whenever we consider energy.
The statement that Fourier transforms conserve energy is
$\bold y'\bold y=\bold x'\bold x$ where $\bold y= \bold F \bold x$.
Substituting gives $\bold F'\, \bold F = \bold I$, which shows that
the inverse matrix to Fourier transform
happens to be the complex conjugate of the transpose of $\bold F$.
\par
With Fourier transforms,
\bx{zero pad}ding and \bx{truncation} are especially prevalent.
Most subroutines transform a dataset of length of $2^n$,
whereas dataset lengths are often of length $m \times 100$.
The practical approach is therefore to pad given data with zeros.
Padding followed by Fourier transformation $\bold F$
can be expressed in matrix algebra as
\begin{equation}
{\rm Program} \eq
\bold F \
 \left[
  \begin{array}{c}
   \bold I \\
   \bold 0
  \end{array}
 \right]
\end{equation}
According to matrix algebra, the transpose of a product,
say $\bold A \bold B = \bold C$,
is the product $\bold C' = \bold B' \bold A'$ in reverse order.
So the adjoint subroutine is given by
\begin{equation}
{\rm Program'} \eq
 \left[
  \begin{array}{cc}
   \bold I & \bold 0
  \end{array}
 \right]
\
\bold F'
\end{equation}
Thus the adjoint subroutine
{\it truncates} the data {\it after} the inverse Fourier transform.
This concrete example illustrates that common sense often represents
the mathematical abstraction
that adjoints of products are reverse-ordered products of adjoints.
It is also nice to see a formal mathematical notation
for a practical necessity.
Making an approximation need not lead to collapse of all precise analysis.


\subsection{Nearest-neighbor coordinates}
\sx{nearest neighbor coordinates}
In describing physical processes,
we often either specify models as values given on a uniform mesh
or we record data on a uniform mesh.
Typically we have
a function $f$ of time $t$ or depth $z$
and we represent it by {\tt f(iz)}
corresponding to $f(z_i)$ for $i=1,2,3,\ldots, n_z$
where $z_i = z_0+ (i-1)\Delta z$.
We sometimes need to handle depth as
an integer counting variable $i$
and we sometimes need to handle it as
a floating-point variable $z$.
Conversion from the counting variable to the floating-point variable
is exact and is often seen in a computer idiom
such as either of \begin{verbatim}
            for (iz=0; iz < nz; iz++) {   z = z0 + iz * dz;
            for (i3=0; i3 < n3; i3++) {  x3 = o3 + i3 * d3;
\end{verbatim}
%{\tt
%   \begin{tabbing}  indent \= \kill
%	\> do iz= 1, nz \{   z = z0 + (iz-1) * dz  \\
%	\> do i3= 1, n3 \{  x3 = o3 + (i3-1) * d3
%   \end{tabbing}
%}
The reverse conversion from the floating-point variable
to the counting variable is inexact.
The easiest thing is to place it at the nearest neighbor.
This is done by solving for {\tt iz}, then adding one half,
and then rounding down to the nearest integer.
The familiar computer idioms are:\begin{verbatim}
        iz = 0.5 + ( z - z0) / dz
        i3 = 0.5 + (x3 - o3) / d3
\end{verbatim}
A small warning is in order:
People generally use positive counting variables.
If you also include negative ones,
then to get the nearest integer,
you should do your rounding with the
C function \texttt{floor}.
%Fortran function {\tt NINT()}.

\subsection{Data-push binning}
\sx{nearest neighbor binning}
\sx{data-push binning}
Binning is putting data values in bins.
Nearest-neighbor binning is an operator.
There is both a forward operator and its adjoint.
Normally the model consists of values given on a uniform mesh,
and the data consists of pairs of numbers (ordinates at coordinates)
sprinkled around in the continuum
(although sometimes the data is uniformly spaced and the model is not).
\par
In both the forward and the adjoint operation,
each data coordinate is examined
and the nearest mesh point (the bin) is found.
For the forward operator,
the value of the bin is added to that of the data.
The adjoint is the reverse:
we add the value of the data to that of the bin.
Both are shown in two dimensions in subroutine \texttt{bin2()}.
\opdex{bin2}{push data into bin}{46}{55}{user/gee}
The most typical application requires an additional step, inversion.
In the inversion applications
each bin contains a different number of data values.
After the adjoint operation is performed,
the inverse operator divides the bin value
by the number of points in the bin.
It is this inversion operator that is generally called binning.
To find the number of data points in a bin,
we can simply apply the adjoint of \texttt{bin2()} to pseudo data of all ones.


\subsection{Linear interpolation}
\inputdir{XFig}
\par
The \bx{linear interpolation}
operator is much like the binning operator but a little fancier.
When we perform the forward operation, we take each data coordinate
and see which two model mesh points bracket it.
Then we pick up the two bracketing model values
and weight each of them
in proportion to their nearness to the data coordinate,
and add them to get the data value (ordinate).
The adjoint operation is adding a data value
back into the model vector;
using the same two weights,
this operation distributes the ordinate value
between the two nearest points in the model vector.
For example, suppose we have a data point near each end of the model
and a third data point exactly in the middle.
Then for a model space 6 points long,
as shown in Figure \ref{fig:helgerud},
we have the operator in (\ref{eqn:lintseq}).
\sideplot{helgerud}{width=3in}{
  Uniformly sampled model space
  and irregularly sampled data space corresponding
  to \protect(\ref{eqn:lintseq}).
}
\begin{equation}
\left[
\begin{array}{c}
  d_0 \\
  d_1 \\
  d_2
  \end{array} \right]
\quad \approx \quad
\left[
\begin{array}{rrrrrr}
   .8 & .2 &  .  & .  & .  & .  \\
   .  & .  &  1  & .  & .  & .  \\
   .  & .  &  .  & .  & .5 & .5
  \end{array} \right]
\left[
	\begin{array}{c}
	  m_0 \\
	  m_1 \\
	  m_2 \\
	  m_3 \\
	  m_4 \\
	  m_5
	\end{array}
\right]
\label{eqn:lintseq}
\end{equation}
The two weights in each row sum to unity.
If a binning operator were used for the same data and model,
the binning operator would contain a ``1.'' in each row.
In one dimension (as here),
data coordinates are often sorted into sequence,
so that the matrix is crudely a diagonal matrix like equation (\ref{eqn:lintseq}).
If the data coordinates covered the model space uniformly,
the adjoint would roughly be the inverse.
Otherwise,
when data values pile up in some places and gaps remain elsewhere,
the adjoint would be far from the inverse.
\par
Subroutine \texttt{lint1()} does linear interpolation and its adjoint.
\opdex{lint1}{linear interp}{45}{59}{user/gee}


\subsection{Causal integration}
\sx{causal integration}
Causal integration is defined as
\begin{equation}
y(t) \eq \int_{-\infty}^t \ x(t)\ dt
\end{equation}
Sampling the time axis gives a matrix equation which
we should call causal summation, but we often call it causal integration.
\begin{equation}
  \left[
	\begin{array}{c}
		y_0 \\
		y_1 \\
		y_2 \\
		y_3 \\
		y_4 \\
		y_5 \\
		y_6 \\
		y_7 \\
		y_8 \\
		y_9 \\
	\end{array}
  \right]
 \quad = \quad
  \left[
	\begin{array}{ccccccccccc}
	1 &0 &0 &0 &0 &0 &0 &0 &0 &0 \\
	1 &1 &0 &0 &0 &0 &0 &0 &0 &0 \\
	1 &1 &1 &0 &0 &0 &0 &0 &0 &0 \\
	1 &1 &1 &1 &0 &0 &0 &0 &0 &0 \\
	1 &1 &1 &1 &1 &0 &0 &0 &0 &0 \\
	1 &1 &1 &1 &1 &1 &0 &0 &0 &0 \\
	1 &1 &1 &1 &1 &1 &1 &0 &0 &0 \\
	1 &1 &1 &1 &1 &1 &1 &1 &0 &0 \\
	1 &1 &1 &1 &1 &1 &1 &1 &1 &0 \\
	1 &1 &1 &1 &1 &1 &1 &1 &1 &1
	\end{array}
  \right]
  \ \
  \left[
	\begin{array}{c}
		x_0 \\
		x_1 \\
		x_2 \\
		x_3 \\
		x_4 \\
		x_5 \\
		x_6 \\
		x_7 \\
		x_8 \\
		x_9 \\
	\end{array}
  \right]
\label{eqn:mytri}
\end{equation}
%Having 1/2 instead of 1 on the diagonal clutters our code a little
%but it is a worthwhile improvement.
%With 1/2, the summation equivalent of
%$\int_{-\infty}^t + \int_t^{\infty} = \int_{-\infty}^\infty$
%remains exactly true.
%Simplifying the 1/2 to a 1 is also accurate if
%we agree to having the integral evaluated half way between two mesh points.
%For most applications, however, it is more convenient to have
%the integral represented on the same mesh as the function itself,
%so we need the 1/2.
(In some applications the 1 on the diagonal is replaced by 1/2.)
Causal integration is
the simplest prototype of a recursive operator.
\sx{recursion, integration}
The coding is trickier than operators we considered earlier.
Notice when you compute $y_5$ that it is the sum of 6 terms,
but that this sum is more quickly computed as $y_5 = y_4 + x_5$.
Thus equation~(\ref{eqn:mytri}) is more efficiently thought of as
the recursion
\begin{equation}
y_t \quad = \quad y_{t-1} + x_t
\quad
\quad
\quad {\rm for\ increasing\ } t
\label{eqn:myrecur}
\end{equation}
(which may also be regarded as a numerical representation
of the \bx{differential equation} $dy/dt=x$.)
\par
When it comes time to think about the adjoint, however,
it is easier to think of equation~(\ref{eqn:mytri}) than of~(\ref{eqn:myrecur}).
Let the matrix of equation~(\ref{eqn:mytri}) be called $\bold C$.
Transposing to get $\bold C '$ and applying it to $\bold y$
gives us something back in the space of $\bold x$,
namely $\tilde{\bold x} = \bold C' \bold y$.
From it we see that the adjoint calculation,
if done recursively,
needs to be done backwards like
\begin{equation}
\tilde x_{t-1} \quad = \quad \tilde x_{t} + y_{t-1}
\quad
\quad
\quad {\rm for\ decreasing\ } t
\label{eqn:backrecur}
\end{equation}
We can sum up by saying that the adjoint of causal integration
is anticausal integration.
\par
A subroutine to do these jobs is \texttt{causint()} \vpageref{lst:causint}.
The code for anticausal integration is not obvious
from the code for integration and the adjoint coding tricks we
learned earlier.
To understand the adjoint, you need to inspect
the detailed form of the expression $\tilde{\bold x} = \bold C' \bold y$
and take care to get the ends correct.
\opdex{causint}{causal integral}{35}{46}{api/c}
\par
%\activesideplot{causint}{width=3.00in,height=3.2in}{ER}{
\inputdir{causint}
\sideplot{causint}{width=3.00in}{
	{\tt in1} is an input pulse.
	{\tt C in1} is its causal integral.
	{\tt C' in1} is the anticausal integral of the pulse.
	{\tt in2} is a separated doublet.
	Its causal integration is a box and its anticausal integration
	is the negative.
	{\tt CC in2} is the double causal integral of {\tt in2}.
	How can an equilateral triangle be built?
	}

\par
Later we will consider equations
to march wavefields up towards the earth's surface,
a layer at a time, an operator for each layer.
Then the adjoint will start from the earth's surface
and march down, a layer at a time, into the earth.
\begin{exer}
\item
Modify the calculation in Figure~\ref{fig:causint} to make
a triangle waveform on the bottom row.
%\item
%Notice that the triangle waveform is not time aligned
%with the input {\tt in2}.
%Force time alignment with the operator
%${\bold C' \bold C}$ or
%${\bold C  \bold C'}$.
%\item
%Modify \GPROG{causint} by changing the diagonal to contain
%1/2 instead of 1.
%Notice how time alignment changes in Figure~\FIG{causint}.
\end{exer}


\section{ADJOINTS AND INVERSES}

Consider a model $\bold m$ and an operator $\bold F$ which creates some
theoretical data $\bold d_{\rm theor}$.
\begin{equation}
	\bold d_{\rm theor} \eq \bold F \bold m
\end{equation}
The general task of geophysicists is to begin from
observed data $\bold d_{\rm obs}$ and
find an estimated model $\bold m_{\rm est}$
that satisfies the simultaneous equations
\begin{equation}
	\bold d_{\rm obs} \eq \bold F \bold m_{\rm est}
\end{equation}
This is the topic of a large discipline variously called
``inversion'' or ``estimation''.
Basically, it defines a residual
$\bold r = \bold d_{\rm obs}-\bold d_{\rm theor}$
and then minimizes its length $\bold r \cdot \bold r$.
Finding $\bold m_{\rm est}$ this way is called
the \bx{least squares} method.
The basic result (not proven here) is that
\begin{equation}
\bold m_{\rm est} = (\bold F'\bold F)^{-1}\bold F'\bold d_{\rm obs}
\end{equation}
In many cases including all seismic imaging cases,
the matrix
$\bold F'\bold F$
is far too large to be invertible.
People generally proceed by a rough guess at an approximation
for $(\bold F'\bold F)^{-1}$.
The usual first approximation is
the optimistic one that $(\bold F'\bold F)^{-1}=\bold I$.
To this happy approximation, the inverse $\bold F^{-1}$
is the adjoint $\bold F'$.

\par
In this book we'll see examples where
$\bold F'\bold F\approx \bold I$
is a good approximation and other examples where it isn't.
We can tell how good the approximation is.
We take some hypothetical data and convert it to a model,
and use that model to make some reconstructed data
$\bold d_{\rm recon} = \bold F \bold F' \bold d_{\rm hypo}$.
Likewise we could go from a hypothetical model to some data and then
to a reconstructed model
$\bold m_{\rm recon} = \bold F' \bold F \bold m_{\rm hypo}$.
Luckily, it often happens that the reconstructed differs from
the hypothetical in some trivial way,
like by a scaling factor, or by a scaling factor
that is a function of physical location or time,
or a scaling factor that is a function of frequency.
It isn't always simply a matter of a scaling factor,
but it often is, and when it is, we often simply
redefine the operator to include the scaling factor.
Observe that there are two places for scaling functions (or filters),
one in model space, the other in data space.

\par
We could do better than the adjoint
%(better than assuming $\bold F'\bold F=\bold I$)
by iterative modeling methods (conjugate gradients)
that are also described elsewhere.
These methods generally demand that the adjoint be computed correctly.
As a result, we'll be a little careful about adjoints in
this book to compute them correctly
even though this book does not require them to be exactly correct.

\subsection{Dot product test}

\par
We define an adjoint when we write a program that computes one.
In an abstract logical mathematical sense, however,
every adjoint is defined by a \bx{dot product test}.
This abstract definition gives us no clues how to code our program.
After we have finished coding, however, this abstract definition
(which is actually a test) has considerable value to us.


\par
Conceptually, the idea of matrix transposition is simply ${a}_{ij}'=a_{ji}$.
In practice, however, we often encounter matrices far too large
to fit in the memory of any computer.
Sometimes it is also not obvious how to formulate the process at hand
as a matrix multiplication.
(Examples are differential equations and fast Fourier transforms.)
What we find in practice is that an application and its adjoint
amounts to two subroutines. The first subroutine
amounts to the matrix multiplication $ \bold F \bold x$.
The adjoint subroutine computes $\bold F' \bold y$,
where $\bold F'$ is the conjugate-transpose matrix.
Most methods of solving inverse problems will fail
if the programmer provides an inconsistent pair of subroutines
for $\bold F$ and $\bold F'$.
The dot product test described next
is a simple test for verifying that the two
subroutines really are adjoint to each other.

\par
The matrix expression
$\bold y' \bold F \bold x $
may be written with parentheses as either
$(\bold y' \bold F) \bold x $ or
$\bold y' (\bold F \bold x)$.
Mathematicians call this the ``associative'' property.
If you write matrix multiplication using summation symbols,
you will notice that putting parentheses around matrices simply
amounts to reordering the sequence of computations.
But we soon get a very useful result.
Programs for some linear operators are far from obvious,
for example \texttt{causint()} \vpageref{lst:causint}.
Now we build a useful test for it.
%The associative property of linear algebra says that
%we do not need parentheses in a vector-matrix-vector product
%like $\bold y' \bold F \bold x $ because we get the same
%result no matter where we put the parentheses.

\begin{eqnarray}
\bold y' ( \bold F \bold x ) &=& ( \bold y' \bold F )  \bold x   \\
\bold y' ( \bold F \bold x ) &=& ( \bold F' \bold y )' \bold x
\label{eqn:bilin2}
\end{eqnarray}
For the dot-product test,
load the vectors $\bold x$ and $\bold y$ with random numbers.
Compute the vector $\tilde \bold y = \bold F\bold x$
using your program for $\bold F$,
and compute
$\tilde \bold x = \bold F'\bold y$
using your program for $\bold F'$.
Inserting these into equation~(\ref{eqn:bilin2})
gives you two scalars that should be equal.
\begin{equation}
\bold y' ( \bold F \bold x ) \eq
\bold y' \tilde \bold y \eq \tilde \bold x ' \bold x
\eq ( \bold F' \bold y )' \bold x
\label{eqn:bilin}
\end{equation}
The left and right sides of this equation will be computationally equal
only if the program doing $\bold F'$ is indeed adjoint
to the program doing $\bold F$
(unless the random numbers do something miraculous).
Note that the vectors $\bold x$ and $\bold y$
are generally of different lengths.

\par
Of course passing the dot product test does not prove that a computer code
is correct, but if the test fails we know the code is incorrect.
More information about adjoint operators,
and much more information about inverse operators
is found in my other books,
Earth Soundings Analysis: Processing versus inversion (PVI) and
Geophysical Estimation by Example (GEE).
